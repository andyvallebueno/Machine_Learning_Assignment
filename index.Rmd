---
title: "Predicting quality of exercise movements using data from accelerometers"
author: "Andy Vallebueno"
date: "February 12, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(parallel)
library(doParallel)
library(dplyr)
library(ggraph)
library(igraph)
setwd("C:/Users/Andy Vallebueno/Documents/DataScience/8_Machine_Learning/Assignment")
```

## Executive summary
This report has the objective of fitting a model that predicts the quality of an activity performed at a specific point in time. It uses the Weight Lifting Exercises Dataset, which investigates how well an activity was performed by the wearer of accelerometers. For this dataset, six participants "were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." While Class A identifies the correct, specified execution of the exercise, the other four classes capture common mistakes made. (Velloso, 2013)

More information on this dataset can be found at the following webpage: http://groupware.les.inf.puc-rio.br/har#dataset#ixzz5fKXfT5aO

## Exploratory Analysis
We begin by reading the training and testing data into R and observe that the training dataset consists of 19,622 observations of 160 variables. These belong to six subjects, as per the user_name variable, producing five different movements (A, B, C, D or E), as per the classe variable. This type of movement is the variable that we shall seek to predict with our model. Below is a summary of the first 20 variables.     

```{r readdata}
training<- read.csv("pml-training.csv")
testing<- read.csv("pml-testing.csv")
summary(training[, 1:20])
```

Given the large size of the training set, we will create a validation set and then proceed to the exploratory analysis on our resulting training set. 

```{r }
set.seed(411)
inTrain<- createDataPartition(y = training$classe, p = 0.8, list = FALSE)
trainingset<- training[inTrain,]; validationset<- training[-inTrain,]
```

We notice that there are several NA values and proceed to count them in order to identify which variables are useful for prediction. We observe that 67 variables have a missing value rate of 97.9%, indicating that these may not be so useful for prediction. Moreover, we observe that in the cases where these variables have a value of "NA", 34 other variables have a blank value. From here on, we will only focus on the remaining variables as there is not information on the previously mentioned variables to use these for prediction. 

```{r }
NAvalues<-NULL
for (i in 1:160) {
  if (mean(is.na(trainingset[,i])) != 0) {
    NAvalues[i]<-i
  } else {NAvalues[i]<-0}
  
}

Blankvalues<-NULL
for (i in 1:160) {
  if (class(trainingset[1, i]) == "factor" & trainingset[1, i] == "") {
    Blankvalues[i]<-i
  } else {
    Blankvalues[i]<-0
    }
}
combined<-c(NAvalues, Blankvalues)
combined<-combined[combined != 0]
newtrainingset<- trainingset[, -combined]
```

We check that we have maintained variables with enough variability with the following zero covariate analysis.  

```{r }
nsv<-nearZeroVar(newtrainingset, saveMetrics = TRUE)
nsv
```

Given the number of variables, we calculate a correlation matrix on the numeric variables. We observe that several variables have a correlation higher than 0.80. 

```{r }
correlations<- abs(cor(newtrainingset[, -c(1, 2 ,3 , 4 ,5 , 6, 60)]))
diag(correlations)<-0
correlations<- as.data.frame(correlations)
subcor<-correlations[correlations > 0.8]
subcor
```

## Model selection 
Since our objective is to predict a factor variable with 5 levels, we will focus on non-linear models. We will start with a classification tree to get an idea of accuracy metrics. We note that this first model (Model 1) has 15,699 nodes and a relatively low accuracy of 66%. As detailed in the confusion matrix, the model correctly classifies A, B and E classes, but incorrectly classifies as E all those observations from the C and D classes. 

```{r }
set.seed(411)
model1<- train(classe ~ . , data = newtrainingset, method = "rpart")
print(model1$finalModel)
model1predictions<- predict(model1, newdata = validationset)
confusionMatrix(validationset$classe, model1predictions)
```

We now fit a second model, a random forest model, to compare. We note that there is an important trade-off here in terms of accuracy and speed compared to our first model. Accuracy for Model 2 increases to 99.9%, although it is much more computationally demanding and subject to overfitting. We note that in this section we are using cross validation as the resampling method in the trainControl function, and changing to 5 the number that specifies the quantity of folds for k-fold cross validation. 

```{r , cache = TRUE}
cluster<-makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl<- trainControl(method = "cv", number = 5, allowParallel = TRUE)
model2<- train(classe ~ . , data = newtrainingset[, 2:60], method = "rf", trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()

print(model2$finalModel)
model2predictions<- predict(model2, newdata = validationset)
confusionMatrix(validationset$classe, model2predictions)
```

For purely visual purposes, we now graph one of the trees from our model to get an idea of how the variables are interacting. We have chosen to plot tree k= 1. Note that the code for this graph, which is purely to illustrate our model, has been sourced from Shirin's playgRound and can be found at this webpage: https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph. Please find full reference in the Sources section.   

```{r , cache = TRUE, echo = FALSE, warning=FALSE}
tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() + ggtitle ("Tree 1") +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
                    repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}

```

```{r , warning=FALSE, fig.height= 8}
tree_func(final_model = model2$finalModel, 1)
```

## Conclusions 
Following an exploratory analysis and model selection process, we have fitted a random forest model with strong accuracy metrics on our validation data set which classifies the quality of a particular activity using 60 variables. This model will be used on a testing set to predict the class or quality of movement of 20 different observations. Although our in sample error is relatively low, we know that the out of sample error or generalization error will be slightly higher, particularly due to overfitting in random forest models. However, we expect an adequate performance.   

## Sources
Glander, Shirin. Plotting trees from Random Forest models with ggraph. Shirin's playgRound. 2019. URL: https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
